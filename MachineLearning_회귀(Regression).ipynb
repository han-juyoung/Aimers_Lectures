{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAFGfmA+IUGZeKW5W+1kNI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/han-juyoung/Aimers_learning/blob/main/MachineLearning_%ED%9A%8C%EA%B7%80(Regression).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4Ac--M7lqij"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from elice_utils import EliceUtils\n",
        "elice_utils = EliceUtils()\n",
        "\n",
        "# 데이터를 생성하고 반환하는 함수입니다.\n",
        "def load_data():\n",
        "\n",
        "    X = np.array([[8.70153760], [3.90825773], [1.89362433], [3.28730045], [7.39333004], [2.98984649], [2.25757240], [9.84450732], [9.94589513], [5.48321616]])\n",
        "    y = np.array([[5.64413093], [3.75876583], [3.87233310], [4.40990425], [6.43845020], [4.02827829], [2.26105955], [7.15768995], [6.29097441], [5.19692852]])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\"\"\"\n",
        "1.  입력값(X)과 beta_0,beta_1를 바탕으로\n",
        "    예측값(pre_y)을 계산하여 반환하는 함수를 구현합니다.\n",
        "\n",
        "    회귀 함수 식을 참고하여\n",
        "    예측값을 계산합니다.\n",
        "\"\"\"\n",
        "# Y = beta_0 + beta_1 * X\n",
        "def prediction(beta_0, beta_1, X):\n",
        "\n",
        "    y_pred = beta_0 + beta_1 * X\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "# beta_0와 beta_1 값을 업데이트 하는 규칙을 정의하는 함수입니다.\n",
        "# lr = 산을 내려갈 때 보폭을 어느정도록 할지\n",
        "def update_beta(X, y, y_pred, lr):\n",
        "\n",
        "    delta_0 = -(lr * (2 / len(X)) * np.sum(y - y_pred)) #beta_0 업데이트\n",
        "\n",
        "    delta_1 = -(lr * (2 / len(X)) * (np.dot(X.T, (y - y_pred)))) #beta_1 업데이트\n",
        "\n",
        "    return delta_0, delta_1\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "2.  반복 횟수만큼 오차(loss)를 계산하고\n",
        "    beta_0,beta_1의 값을 변경하는 함수를 구현합니다.\n",
        "\n",
        "    Step01. 실제 값 y와 prediction 함수를 통해 예측한\n",
        "    예측 값 pred_y 간의 차이(loss)를 계산합니다.\n",
        "\n",
        "    loss는 실제값(y) - 예측값(pred_y)으로 정의하겠습니다.\n",
        "\n",
        "    Step02. 구현된 함수를 이용하여\n",
        "    beta_0와 beta_1 의 변화값을 각각 beta0_delta, beta1_delta에 저장합니다.\n",
        "\"\"\"\n",
        "def gradient_descent(X, y, iters, lr):\n",
        "    # np.zeros((1,1)) shape를 뜻한다. 값이 1개라는 뜻.\n",
        "    beta_0 = np.zeros((1,1)) # 초기값을 0으로 설정\n",
        "    beta_1 = np.zeros((1,1))\n",
        "\n",
        "    for i in range(iters): # 이 iters 횟수만큼 학습진행\n",
        "        # loss = y - prediction(beta_0, beta_1, X) # 예측값 계산\n",
        "        y_pred = prediction(beta_0, beta_1, X)\n",
        "        loss = np.mean(np.square(y - y_pred))\n",
        "\n",
        "        beta0_delta, beta1_delta = update_beta(X, y, y_pred, lr)\n",
        "\n",
        "        beta_0 -= beta0_delta\n",
        "        beta_1 -= beta1_delta\n",
        "\n",
        "        # 100번의 학습마다 그래프 출력하기\n",
        "        if i%100==0:\n",
        "            print(\"학습 횟수 :\",i)\n",
        "            plotting_graph(X,y,beta_0,beta_1)\n",
        "\n",
        "    return beta_0, beta_1 # 최적의 값 반환\n",
        "\n",
        "\n",
        "# 그래프를 시각화하는 함수입니다.\n",
        "def plotting_graph(X,y,beta_0,beta_1):\n",
        "\n",
        "    y_pred = beta_0 + beta_1[0,0] * X\n",
        "\n",
        "    fig = plt.figure()\n",
        "\n",
        "    plt.scatter(X, y)\n",
        "    plt.plot(X, y_pred,c='r')\n",
        "\n",
        "    plt.savefig(\"test.png\")\n",
        "    elice_utils.send_image(\"test.png\")\n",
        "\n",
        "\n",
        "# 회귀 알고리즘 구현 진행을 위한 main() 함수입니다.\n",
        "def main():\n",
        "\n",
        "    # 학습을 위해 필요한 파라미터입니다.\n",
        "    lr = 1e-4\n",
        "    iteration = 1000\n",
        "\n",
        "    X, y = load_data()\n",
        "\n",
        "    beta_0, beta_1 = gradient_descent(X, y, iteration, lr)\n",
        "\n",
        "    print(\"{}번의 학습 이후의 회귀 알고리즘 결과\".format(iteration))\n",
        "    print(\"beta_0:\",beta_0[0], \"beta_1:\",beta_1[0])\n",
        "\n",
        "    plotting_graph(X,y,beta_0,beta_1)\n",
        "\n",
        "    return beta_0, beta_1\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ]
    }
  ]
}